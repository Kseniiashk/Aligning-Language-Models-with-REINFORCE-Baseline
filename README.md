# Aligning-Language-Models-with-REINFORCE-Baseline

Ответ на На данном этапе НЕ рекомендуется использовать RLOOTrainer из. библиотеки trl, так как реализация в нём отличается от алгоритма, описанного в статье (Bonus: в отчёте опишите почему именно) ?

Baseline — это просто скользящее среднее наград, которое не требует градиентов или дополнительных параметров. Это снижает вычислительные затраты и упрощает реализацию. А RLOOTrainer является ценностной сетью, которая обучается предсказывать ожидаемую награду для текущего состояния. Из-за чего вводится дополнительная модель, которую нужно обучать и повышается риск ошибок из-за неточности предсказаний Value Network, да и просто усложняется процесс. RLOOTrainer реализация ближе к PPO или другим продвинутым методам RL, а REINFORCE w/ baseline — это одно этапный алгоритм.

Средняя награда  на отложенной выборке (validation split) выросла после обучения  по сравнению c SFT моделью.
