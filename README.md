# Aligning-Language-Models-with-REINFORCE-Baseline

### Отчет об эксперименте по обучению модели с использованием алгоритма REINFORCE

#### 1. **Цель нашего эксперимента**
Основная цель эксперимента заключалась в улучшении качества генерации текста с помощью модели SFT путем применения алгоритма обучения с подкреплением REINFORCE. Для оценки качества генерации использовали модель RewardModel, которая оценивала сгенерированные тексты и выдавала числовую оценку. Целью было увеличить среднюю награду, получаемую от RewardModel.

#### 2. **Исходные данные и инструменты**
- **Модель SFT**: использовалась модель "HuggingFaceTB/SmolLM2-135M-Instruct" для генерации текстов.
- **RewardModel**: модель, обученная для оценки качества сгенерированных текстов.
- **Данные**: использовался датасет "esfrankel17/HelpSteer2_binarized", разделенный на обучающую и валидационную выборки в соотношении 80/20.
- **Инструменты**: использовались библиотеки Transformers, TRL, Datasets, Accelerate, PyTorch и другие.
Ответ на вопрос: "На данном этапе НЕ рекомендуется использовать RLOOTrainer из. библиотеки trl, так как реализация в нём отличается от алгоритма, описанного в статье (Bonus: в отчёте опишите почему именно)" ?

Baseline — это просто скользящее среднее наград, которое не требует градиентов или дополнительных параметров. Это снижает вычислительные затраты и упрощает реализацию. А RLOOTrainer является ценностной сетью, которая обучается предсказывать ожидаемую награду для текущего состояния. Из-за чего вводится дополнительная модель, которую нужно обучать и повышается риск ошибок из-за неточности предсказаний Value Network, да и просто усложняется процесс. RLOOTrainer реализация ближе к PPO или другим продвинутым методам RL, а REINFORCE w/ baseline — это одно этапный алгоритм.

#### 3. **Процесс обучения**
1. **Предобработка данных**: данные были предобработаны с помощью токенизатора, чтобы привести их к формату, подходящему для модели.
2. **Инициализация модели и оптимизатора**: модель SFT и RewardModel были загружены и перенесены на устройство (GPU). Для обучения использовался оптимизатор Adam с learning rate 1e-5.
3. **Обучение с использованием REINFORCE**: в цикле обучения модель SFT генерировала тексты на основе входных данных. Затем RewardModel оценивала эти тексты, и на основе полученных оценок вычислялись потери (loss). Обновление весов модели происходило с учетом этих потерь.
4. **Оценка модели**: после каждой эпохи обучения проводилась оценка модели на валидационной выборке. Оценка проводилась с помощью RewardModel, и вычислялась средняя награда.

#### 4. **Результаты**
- **До обучения**: средняя награда, полученная от RewardModel на валидационной выборке, составила **-34.715**.
- **После обучения**: средняя награда после одной эпохи обучения с использованием REINFORCE составила **-31.7**.

#### 5. **Анализ результатов**
- **Улучшение**: наблюдается улучшение средней награды на **3.015** (с -34.715 до -31.7). Это указывает на то, что модель SFT стала генерировать тексты, которые RewardModel оценивает как более качественные.
- **Значимость**: хотя улучшение является положительным, абсолютное значение награды остается отрицательным, что может указывать на то, что модель еще не достигла желаемого уровня качества. Возможно, требуется больше эпох обучения или более сложная настройка гиперпараметров.

#### 6. **Выводы и рекомендации**
- **Эффективность REINFORCE**: алгоритм REINFORCE показал свою эффективность в улучшении качества генерации текста, что подтверждается увеличением средней награды.
- **Дальнейшие шаги**: для дальнейшего улучшения модели можно рассмотреть следующие шаги:
  - Увеличить количество эпох обучения.
  - Провести более тщательный подбор гиперпараметров, таких как learning rate и batch size.
  - Использовать более сложные модели RewardModel или дополнительные метрики для оценки качества.

#### 7. **Заключение**
Эксперимент показал, что использование алгоритма REINFORCE может улучшить качество генерации текста. Однако для достижения более высоких результатов требуется дополнительная работа по настройке и оптимизации модели.
